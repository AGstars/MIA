黑盒攻击（Black-Box Setting）
在黑盒攻击中，假设攻击者只能访问模型的预测结果，而不能查看模型的内部架构和参数。

生成影子模型：需要在可用的数据集上训练多个影子模型，这些模型模拟目标模型的行为
收集预测结果：对已知的训练数据和未见数据分别进行预测，收集这些预测结果
训练攻击模型：使用影子模型生成的预测结果作为训练数据，训练一个攻击模型。这个攻击模型的目标是根据预测结果来判断某个数据点是否在训练集中。

黑盒攻击的关键在于模拟目标模型的行为，并使用大量的影子模型来增强攻击模型的准确性。

高准确率：如果攻击模型的准确率接近 1.0，说明攻击模型能够较好地区分成员和非成员。这意味着原始模型可能存在较高的隐私泄露风险，因为攻击者能够有效地判断哪些数据是用于训练的。
低准确率：如果准确率接近 0.5，说明攻击模型的判断接近随机猜测，即攻击模型无法有效区分成员和非成员。这表明原始模型的隐私泄露风险较低。

在写定的成员推断攻击代码中，成员（即训练数据）和非成员（即测试数据）分别是：
成员（训练数据）：用于训练原始模型的样本。在代码中，这些样本来自于 train_dataset，并被拆分成 train_dataset 和 shadow_dataset。用于训练原始模型的样本是 train_dataset 的一部分。
非成员（测试数据）：未用于训练原始模型的样本。在代码中，这些样本来自于 test_dataset。

成员（训练数据）：
train_dataset：这是从原始 train_dataset 中拆分出来的一部分。代码中使用 torch.utils.data.random_split 方法将原始 train_dataset 分成两部分，其中一部分用于训练原始模型，另一部分用于影子模型。
shadow_loader：数据加载器 shadow_loader 加载了 shadow_dataset，这是用于训练影子模型的成员数据。

非成员（测试数据）：
test_dataset：这是整个 test_dataset，包含了未用于训练原始模型的数据。
test_loader：数据加载器 test_loader 加载了 test_dataset，这是用于影子模型和攻击模型评估的非成员数据。
